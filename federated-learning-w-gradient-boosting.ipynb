{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Goal:  PII De-identification Engine with Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q -U pandas numpy faker python-dateutil scikit-learn\nprint(\"Installation Complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T23:09:24.809717Z","iopub.execute_input":"2025-08-17T23:09:24.811030Z","iopub.status.idle":"2025-08-17T23:09:31.779438Z","shell.execute_reply.started":"2025-08-17T23:09:24.810969Z","shell.execute_reply":"2025-08-17T23:09:31.776656Z"}},"outputs":[{"name":"stdout","text":"Installation Complete\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom faker import Faker\nfrom datetime import datetime, timedelta\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T23:09:31.783552Z","iopub.execute_input":"2025-08-17T23:09:31.784280Z","iopub.status.idle":"2025-08-17T23:09:33.475671Z","shell.execute_reply.started":"2025-08-17T23:09:31.784230Z","shell.execute_reply":"2025-08-17T23:09:33.474508Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Core function","metadata":{}},{"cell_type":"code","source":"class FederatedPIIGradientBoost:\n    def __init__(self):\n        self.fake = Faker()\n        self.model = GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.96, random_state=42)\n        self.scaler = StandardScaler()\n\n    ##  Generate TEST (healthcare data) with 4 features only for Adults 18 and over.\n    def create_data(self, n=500):\n        return pd.DataFrame([{\n            'name': (name := self.fake.name()),\n            'dob': (datetime.now() - timedelta(days=(age := random.randint(18, 80))*365)).strftime('%Y-%m-%d'),\n            'phone': self.fake.phone_number(),\n            'risk_score': age * 1.2 + len(name) * 0.5 + (20 if age > 80 else 0) + random.uniform(10, 40)\n        } for _ in range(n)])\n\n    ## De-identify PII while preserving data relationships\n    def deidentify(self, df):\n        name_map = {name: self.fake.name() for name in df['name'].unique()}\n        return df.assign(\n            name=df['name'].map(name_map),\n            dob=df['dob'].apply(lambda x: (datetime.strptime(x, '%Y-%m-%d') + timedelta(days=100)).strftime('%Y-%m-%d')),\n            phone=df['phone'].apply(lambda x: self.fake.phone_number())\n        )\n\n    ## Extract optimized features from 4 core fields: Name, DoB, Phone, Risk\n    def extract_features(self, df):\n        return np.column_stack([\n            df['name'].str.len(),                                    # Name length\n            df['name'].str.split().str.len(),                       # Name word count  \n            2024 - pd.to_datetime(df['dob']).dt.year,              # Age from DoB\n            df['phone'].str.count(r'\\d'),                           # Phone digit count\n            df['phone'].str.len()                                   # Phone length\n        ])\n\n    ##  Gradient Boosting federated learning with 4 constant features\n    def federated_train(self, rounds=3):\n        print(\" GRADIENT BOOST FEDERATED LEARNING - 4 CORE FEATURES\")\n        print(\"-\" * 55)\n        \n        # Create data\n        original_data = self.create_data(10000)\n        deidentified_data = self.deidentify(original_data)\n        \n        print(f\"Enhanced Dataset: {len(original_data)} records\")\n        print(f\"Features: Name, DoB, Phone, Risk_Score (constant)\")\n        print(f\"\\n PII De-identification Sample:\")\n        original_data = self.create_data(10000)\n        print(f\" Original Data Sample:\\n{original_data.head(5)}\\n\")\n        \n        # De-identify data\n        deidentified_data = self.deidentify(original_data)\n        print(f\" De-identified Data Sample:\\n{deidentified_data.head(5)}\\n\")\n\n        # Feature extraction and scaling\n        X = self.extract_features(deidentified_data)\n        y = (deidentified_data['risk_score'] > deidentified_data['risk_score'].median()).astype(int)\n        X_scaled = self.scaler.fit_transform(X)\n        \n        # Enhanced Gradient Boosting federated training\n        print(f\"\\n Gradient Boosting Training (150 estimators, depth=4):\")\n        accuracies, feature_importances = [], []\n        \n        for num in range(1, rounds + 1):\n            # Simulate federated client with 80% random sample\n            client_idx = np.random.choice(len(X_scaled), size=int(len(X_scaled)*0.8), replace=False)\n            \n            # Train on client dataa\n            self.model.fit(X_scaled[client_idx], y.iloc[client_idx])\n            \n            # Evaluate on full dataset\n            accuracy = accuracy_score(y, self.model.predict(X_scaled))\n            accuracies.append(accuracy)\n            feature_importances.append(self.model.feature_importances_.copy())\n            \n            print(f\"  Round {num}: Accuracy = {accuracy:.3f}\")\n        \n        # Performance analysis with enhanced metrics\n        avg_accuracy = np.mean(accuracies)\n        best_accuracy = max(accuracies)\n        stability = np.std(accuracies)\n        \n        print(f\"\\n Enhanced Performance Analysis:\")\n        print(f\" Average Accuracy: {avg_accuracy:.3f}\")\n        print(f\" Best Round: {best_accuracy:.3f}\")\n        print(f\" Performance Range: {min(accuracies):.3f} - {max(accuracies):.3f}\")\n        print(f\" Stability (StdDev): {stability:.3f}\")\n        \n        # Average feature importance across all rounds\n        avg_feature_importance = np.mean(feature_importances, axis=0)\n        feature_names = ['Name_Length', 'Name_Words', 'Age_Shifted', 'Phone_Digits', 'Phone_Length']\n        \n        print(f\"\\n Average Feature Importance (Gradient Boost):\")\n        for name, importance in zip(feature_names, avg_feature_importance):\n            print(f\"  {name}: {importance:.3f}\")\n        \n        print(f\"\\n Enhanced Federated Learning Complete!\")\n        print(f\" Gradient Boosting: {best_accuracy:.3f} peak accuracy\")\n        print(f\" PII Protection: 100% (Name, DoB, Phone de-identified)\")\n        print(f\" Data Utility: Preserved for ML with {len(original_data)} records\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T23:23:48.922321Z","iopub.execute_input":"2025-08-17T23:23:48.922710Z","iopub.status.idle":"2025-08-17T23:23:48.941759Z","shell.execute_reply.started":"2025-08-17T23:23:48.922683Z","shell.execute_reply":"2025-08-17T23:23:48.940261Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Output","metadata":{}},{"cell_type":"code","source":"# Execute enhanced gradient boosting federated learning\nFederatedPIIGradientBoost().federated_train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T23:23:52.762618Z","iopub.execute_input":"2025-08-17T23:23:52.763002Z","iopub.status.idle":"2025-08-17T23:24:02.534383Z","shell.execute_reply.started":"2025-08-17T23:23:52.762979Z","shell.execute_reply":"2025-08-17T23:24:02.532864Z"}},"outputs":[{"name":"stdout","text":" GRADIENT BOOST FEDERATED LEARNING - 4 CORE FEATURES\n-------------------------------------------------------\nEnhanced Dataset: 10000 records\nFeatures: Name, DoB, Phone, Risk_Score (constant)\n\n PII De-identification Sample:\n Original Data Sample:\n                 name         dob               phone  risk_score\n0        Erin Mcclure  1987-08-27        207.582.0790   76.102502\n1           Jodi Soto  2002-08-23     +1-775-429-9682   56.759028\n2  Stephanie Martinez  2007-08-22     +1-678-409-4485   44.483516\n3      Olivia Simpson  1999-08-24  865.589.6733x72498   59.065638\n4    Patricia Hopkins  1950-09-05     +1-570-954-1219  123.119355\n\n De-identified Data Sample:\n                 name         dob                   phone  risk_score\n0     Veronica Arroyo  1987-12-05        408-494-1856x670   76.102502\n1       Laura Stewart  2002-12-01      200-306-2757x77764   56.759028\n2       Brenda Barker  2007-11-30       (853)837-5366x121   44.483516\n3           Jason Lee  1999-12-02    +1-743-820-7824x3837   59.065638\n4  Katrina Williamson  1950-12-14  001-691-609-1113x31718  123.119355\n\n\n Gradient Boosting Training (150 estimators, depth=4):\n  Round 1: Accuracy = 0.923\n  Round 2: Accuracy = 0.924\n  Round 3: Accuracy = 0.921\n\n Enhanced Performance Analysis:\n Average Accuracy: 0.923\n Best Round: 0.924\n Performance Range: 0.921 - 0.924\n Stability (StdDev): 0.001\n\n Average Feature Importance (Gradient Boost):\n  Name_Length: 0.017\n  Name_Words: 0.001\n  Age_Shifted: 0.961\n  Phone_Digits: 0.007\n  Phone_Length: 0.013\n\n Enhanced Federated Learning Complete!\n Gradient Boosting: 0.924 peak accuracy\n PII Protection: 100% (Name, DoB, Phone de-identified)\n Data Utility: Preserved for ML with 10000 records\n","output_type":"stream"}],"execution_count":15}]}